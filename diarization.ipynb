{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kj2WnnxNsUf",
        "outputId": "88a77f96-8ceb-4bea-f4e5-b8877da9fc46"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMXbhFDcxzDa"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_dPAIJKw_G_",
        "outputId": "440bc820-9e39-49ff-fccc-3b67c458b304"
      },
      "outputs": [],
      "source": [
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2tSpG56CjMMN",
        "outputId": "09190853-ad79-439f-ddd4-b110f1ccb013"
      },
      "outputs": [],
      "source": [
        "!pip install pyannote.audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxBZ794JaDdM"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he4T4ZIZaDdN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from pyannote.audio import Pipeline\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE3OX9zXnAjD",
        "outputId": "24fa5395-de4b-452d-9742-33ac9a27369b"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline.from_pretrained(\n",
        "    \"pyannote/speaker-diarization-3.1\",\n",
        "    use_auth_token=\"hugging face authentication token\")\n",
        "\n",
        "# send pipeline to GPU (when available)\n",
        "import torch\n",
        "pipeline.to(torch.device(\"cuda\"))\n",
        "files = os.listdir(\"path to the audio files\")\n",
        "print(len(files))\n",
        "for i in files:\n",
        "  diarization = pipeline(\"path to the audio files\" + i)\n",
        "  list_of_dict = []\n",
        "  index = 0\n",
        "  name, ext = os.path.splitext(i)\n",
        "  os.makedirs(f\"path where you wanna save the partitions of each file/{name}\", exist_ok=True)\n",
        "  # print the result\n",
        "  for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "      list_of_dict.append({\"start\":turn.start, \"stop\":turn.end, \"speaker\":speaker , \"transcript\":None})\n",
        "      current_audio = AudioSegment.from_wav(\"path to the audio files\" + i)\n",
        "      current_audio = current_audio[int(turn.start*1000):int(turn.end*1000)]\n",
        "      current_audio.export(\"path where you wanna save the partitions of each file/\" + name + f\"/{index}.wav\", format=\"wav\")\n",
        "      index += 1\n",
        "  json_file = open(f\"path where you wanna save the partitions of each file/{name}/{name}.json\", \"w\")\n",
        "  json.dump(list_of_dict, json_file)\n",
        "  json_file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlFYUx00D4ry",
        "outputId": "d5ce83b5-83de-45c0-a31f-0536568abe54"
      },
      "outputs": [],
      "source": [
        "len(os.listdir(\"path where you wanna save the partitions of each file\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snet44lfaDdT",
        "outputId": "3ab17079-f2c0-49e9-817f-845e50bdc840"
      },
      "outputs": [],
      "source": [
        "# The set of characters accepted in the transcription.\n",
        "characters = [x for x in \"غظضذخثةتشقرصفعسمنلكيطحزوؤهدجبىائءإآأ \"]\n",
        "# Mapping characters to integers\n",
        "char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n",
        "# Mapping integers back to original characters\n",
        "num_to_char = keras.layers.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
        "    f\"(size ={char_to_num.vocabulary_size()})\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmvYbUEXaDdT"
      },
      "source": [
        "Next, we create the function that describes the transformation that we apply to each\n",
        "element of our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzEJdqc4aDdU"
      },
      "outputs": [],
      "source": [
        "# An integer scalar Tensor. The window length in samples.\n",
        "frame_length = 240 # previously 256 400\n",
        "# An integer scalar Tensor. The number of samples to step.\n",
        "frame_step = 120 # previously 160 200\n",
        "# An integer scalar Tensor. The size of the FFT to apply.\n",
        "# If not provided, uses the smallest power of 2 enclosing frame_length. /////////////////////////////\n",
        "\n",
        "fft_length = 256 # previously 384\n",
        "\n",
        "sample_rate = 16000\n",
        "def encode_single_sample(wav_file):\n",
        "    ###########################################\n",
        "    ##  Process the Audio\n",
        "    ##########################################\n",
        "    # 1. Read wav file\n",
        "    file = tf.io.read_file(wavs_path + wav_file + \".wav\")\n",
        "    # 2. Decode the wav file\n",
        "    audio, _ = tf.audio.decode_wav(file)\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    # 3. Change type to float\n",
        "    audio = tf.cast(audio, tf.float32)\n",
        "    # 4. Get the spectrogram\n",
        "    stfts = tf.signal.stft(\n",
        "        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
        "    )\n",
        "\n",
        "    # 5. We only need the magnitude, which can be derived by applying tf.abs\n",
        "    spectrogram = tf.abs(stfts)\n",
        "    spectrogram = tf.math.pow(spectrogram, 0.5)\n",
        "    # 6. normalisation\n",
        "    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
        "    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
        "    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
        "    return spectrogram   #spectrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4BQyb6taDda"
      },
      "source": [
        "## Model\n",
        "\n",
        "We first define the CTC Loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BBrp4W9aDdb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def CTCLoss(y_true, y_pred):\n",
        "    # Compute the training-time loss value\n",
        "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqiccPRjaDdb"
      },
      "source": [
        "We now define our model. We will define a model similar to\n",
        "[DeepSpeech2](https://nvidia.github.io/OpenSeq2Seq/html/speech-recognition/deepspeech2.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkSvWCYdaDdc",
        "outputId": "845e2a45-bbaf-4bbe-ce68-25fb2d91f0b2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_model(input_dim, output_dim, rnn_layers= 5, rnn_units=256):\n",
        "    \"\"\"Model similar to DeepSpeech2.\"\"\"\n",
        "    # Model's input\n",
        "    input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n",
        "    # Expand the dimension to use 2D CNN.\n",
        "    x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\")(input_spectrogram)\n",
        "\n",
        "     # Convolutional layers\n",
        "    for i, (filters, kernel_size, strides) in enumerate(\n",
        "        [(96, [11, 41], [2, 2]), (128, [11, 21], [1, 2])\n",
        "        ]\n",
        "    ):\n",
        "        x = layers.Conv2D(\n",
        "            filters=filters,\n",
        "            kernel_size=kernel_size,\n",
        "            strides=strides,\n",
        "            padding=\"same\",\n",
        "            use_bias=False,\n",
        "            name=f\"conv_{i+1}\",\n",
        "            kernel_initializer=tf.initializers.GlorotUniform(),\n",
        "        )(x)\n",
        "        x = layers.ReLU(name=f\"conv_{i+1}_relu\")(x)\n",
        "\n",
        "    # Reshape the resulted volume to feed the RNNs layers\n",
        "    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
        "    # RNN layers\n",
        "    for i in range(1, rnn_layers + 1):\n",
        "        recurrent = layers.GRU(\n",
        "            units=rnn_units,\n",
        "            activation=\"tanh\",\n",
        "            recurrent_activation=\"sigmoid\",\n",
        "            use_bias=True,\n",
        "            return_sequences=True,\n",
        "            reset_after=True,\n",
        "            name=f\"gru_{i}\",\n",
        "            kernel_initializer=tf.initializers.GlorotUniform(),\n",
        "        )\n",
        "        x = layers.Bidirectional(\n",
        "            recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n",
        "        )(x)\n",
        "\n",
        "    # Dense layer\n",
        "    x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(x)\n",
        "    x = layers.ReLU(name=\"dense_1_relu\")(x)\n",
        "\n",
        "    output = layers.Dense(units=output_dim + 1, activation=\"softmax\")(x)\n",
        "    model = keras.Model(input_spectrogram, output, name=\"DeepSpeech_2\")\n",
        "    # Optimizer\n",
        "\n",
        "    opt = keras.optimizers.Adam(learning_rate=1e-5)\n",
        "    # Compile the model and return\n",
        "    model.compile(optimizer=opt, loss=CTCLoss)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Get the model\n",
        "model = build_model(\n",
        "    input_dim=fft_length // 2 + 1,\n",
        "    output_dim=char_to_num.vocabulary_size(),\n",
        "    rnn_units=768,\n",
        ")\n",
        "model.summary(line_length=110)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcqyQoJxhUey"
      },
      "outputs": [],
      "source": [
        "model.load_weights(\"./model_01_224.15.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M95bpHbiaDdd"
      },
      "outputs": [],
      "source": [
        "# A utility function to decode the output of the network\n",
        "def decode_batch_predictions(pred):\n",
        "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "    # Use greedy search. For complex tasks, you can use beam search\n",
        "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True, beam_width=512)[0][0]\n",
        "    # Iterate over the results and get back the text\n",
        "    output_text = []\n",
        "    for result in results:\n",
        "        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
        "        output_text.append(result)\n",
        "    return output_text\n",
        "\n",
        "\n",
        "# A callback class to output a few transcriptions during training\n",
        "class CallbackEval(keras.callbacks.Callback):\n",
        "    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs=None):\n",
        "        predictions = []\n",
        "        targets = []\n",
        "        for batch in self.dataset:\n",
        "            X, y = batch\n",
        "            batch_predictions = model.predict(X)\n",
        "            batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "            predictions.extend(batch_predictions)\n",
        "            for label in y:\n",
        "                label = (\n",
        "                    tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "                )\n",
        "                targets.append(label)\n",
        "            break\n",
        "\n",
        "        for i in np.random.randint(0, len(predictions), 32):\n",
        "            print(f\"Target    : {targets[i]}\")\n",
        "            print(f\"Prediction: {predictions[i]}\")\n",
        "            print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ejlcRWOuaDdP",
        "outputId": "0a9736ab-6914-41b9-9a28-3c33cbece064"
      },
      "outputs": [],
      "source": [
        "files = os.listdir(\"path to the audio files\") #Add your path here\n",
        "print(len(files))\n",
        "i = 0\n",
        "for i in files:\n",
        "  name, ext = os.path.splitext(i)\n",
        "  wavs_path = f\"path where you want to save the partitions of each file/{name}/\" #Add your path here like this: f\"home/data/intervals/{name}/\"\n",
        "  wavs = os.listdir(wavs_path)\n",
        "  wavs.remove(f\"{name}.json\")\n",
        "  for i in range(len(wavs)):\n",
        "    wavs[i] = wavs[i].split(\".\")[0]\n",
        "  df = pd.DataFrame({\n",
        "      'audio': wavs,\n",
        "  })\n",
        "  print(df)\n",
        "  batch_size = 32\n",
        "  df = tf.data.Dataset.from_tensor_slices(\n",
        "      (np.array(df[\"audio\"].tolist()))\n",
        "  )\n",
        "  intervals = (\n",
        "      df.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .padded_batch(batch_size)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "  )\n",
        "  predictions = []\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    for batch in intervals:\n",
        "        X = batch\n",
        "        batch_predictions = model.predict(X)\n",
        "        batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "        predictions.extend(batch_predictions)\n",
        "  list_of_dicts = []\n",
        "\n",
        "  #Add your path here\n",
        "  with open(f'path where you want to save the partitions of each file/{name}/{name}.json', 'r', encoding='utf-8') as file:\n",
        "    list_of_dicts = json.load(file)\n",
        "  file.close()\n",
        "  for i in range (len(list_of_dicts)):\n",
        "    list_of_dicts[i][\"transcript\"] = predictions[i]\n",
        "    print(list_of_dicts[i])\n",
        "\n",
        "  #Add your path here\n",
        "  with open(f\"path where you want to save the jsons/{name}.json\", 'w', encoding='utf-8') as file:\n",
        "    json.dump(list_of_dicts, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "  file.close()\n",
        "  print(name)\n",
        "  print(i)\n",
        "  i += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
